\documentclass[DM,lsstdraft,STS,toc]{lsstdoc}
\usepackage{enumitem}
\input meta.tex

\begin{document}

\def\product{LSST Level 1 System}

\setDocCompact{true}

\title[Test Spec for \product]{\product~Test Specification}

\author{Eric C. Bellm}
\setDocRef{\lsstDocType-\lsstDocNum}
\setDocDate{\vcsdate}

\setDocAbstract {
This document describes the detailed test specification for the \product{}.
}

% Most recent last
\setDocChangeRecord{%
	\addtohist{1}{2017-11-XX}{Initial release of draft version.}{Bellm}
}

\setDocCurator{Eric C. Bellm}
\setDocUpstreamLocation{\url{https://github.com/lsst/ldm-533}}
\setDocUpstreamVersion{\vcsrevision}

\maketitle

\section{Introduction}
\label{sec:intro}

This document specifies the test procedure for the \product{}.

The \product{} is the component of the LSST system which is responsible for
scientific processing leading to:

\begin{itemize}

  \item{Single frame processing and measurement;}
  \item{Alert generation from difference image analysis;}
  \item{Alert distribution to community brokers;}
  \item{Simple filtering of alerts;}
  \item{Precovery and forced photometry measurements on new and previously-known
	  sources found in difference imaging;}
  \item{Identification of moving objects.}
  \item{Generating QC metrics based on pipeline execution and post-processing of
  scientific data products.}


\end{itemize}

A full description of this product is provided in \S6 (which describes the
Data Facility-provided execution services) and \S13.1 (the science payloads) of
\citeds{LDM-148}.

\subsection{Objectives}
\label{sec:objectives}

This document builds on the description of LSST Data Management's approach to
testing as described in \citeds{LDM-503} to describe the detailed tests that
will be performed on the \product{} as part of the verification of the DM system.

It identifies test designs, test cases and procedures for the tests, and the
pass/fail criteria for each test.

\subsection{Scope}
\label{sec:scope}

This document describes the test procedures for the following components of
the LSST system (as described in \citeds{LDM-148}):

\begin{itemize}

  \item{Services provided by the LSST Data Facility:

    \begin{itemize}
      \item{Prompt Processing Execution}
      \item{Batch and Offline Processing Execution}
      \item{Level 1 Quality Control}
      \item{Alert Distribution Execution}
      \item{Alert Filtering Execution}
    \end{itemize}
  }

  \item{Science payloads:

    \begin{itemize}
      \item{Single frame processing Payload}
      \item{Alert Generation Payload}
      \item{Precovery and Forced Photometry Payload}
      \item{MOPS Payload}
    \end{itemize}

  }

\end{itemize}

\subsection{Applicable Documents}
\label{sec:docs}

\addtocounter{table}{-1}

\begin{tabular}[htb]{l l}
\citeds{LDM-148} & LSST DM System Architecture \\
\citeds{LDM-151} & LSST DM Science Pipelines Design \\
\citeds{LDM-294} & LSST DM Organization \& Management \\
\citeds{LDM-502} & The Measurement and Verification of DM Key Performance Metrics \\
\citeds{LDM-503} & LSST DM Test Plan \\
\citeds{LSE-61}  & LSST DM Subsystem Requirements \\
\citeds{LSE-163} & LSST Data Products Definition Document \\
\end{tabular}

\subsection{References\label{sect:references}}
\renewcommand{\refname}{}
\bibliography{lsst,refs,books,refs_ads}

%\subsection{Definitions, acronyms, and abbreviations \label{sect:acronyms}} % include acronyms.tex generated by the acronyms.csh (GaiaTools)
%\input{acronyms}


%----------------------------------------------------
% TASK IDENTIFICATION - APPROACH
%----------------------------------------------------
\section{Approach}
\label{sec:approach}

The major activities to be performed are to:

\begin{itemize}

  \item{Compare the design of the Alert Production payload as
  implemented to the requirements on the outputs of the DM Subsystem as
  defined in \citeds{LSE-63} and \citeds{LSE-163} to demonstrate that all data
  products required by the scientific community will be delivered by the
  system as built.}

  \item{Ensure that all data products included in the AP payload design are
  correctly produced and persisted appropriately to the LSST 
  Data Backbone, Alert Distribution System, and/or Alert Filtering service
  as appropriate.}

  \item{Ensure that all data products required by the Precovery and Forced
  Photometry payload are correctly
  produced and persisted appropriately to the LSST Data Backbone.}

  \item{Ensure that all data products required by the MOPS system are correctly
  produced and persisted appropriately to the LSST Data Backbone.}

  \item{Demonstrate that QC metrics are properly calculated and transmitted
  during the execution all L1 production types.}

  \item{Demonstrate that post-processing QC analysis of data products can be
  used to identify and report on failures or anomalies in the processing.}

\end{itemize}

\subsection{Tasks and criteria}
\label{sec:tasks}

The following are the major items under test:

\begin{itemize}

  \item{The science payload capable of prompt processing of single visit
	  images;}

  \item{The Alert Generation payload that detects variable sources through
	  difference image analysis;}

  \item{The Alert Distribution System that packages alerts and forwards
	  them to community brokers;}

  \item{The filtering system that allows science users to apply simple
	  filters to the alert stream;} 


  \item{The Precovery and Forced Photometry payloads that measure flux
	  levels for new and previously-known sources found in difference
	  images;}
	
  \item{The Moving Object Processing System payload that identifies solar
	  system bodies from difference image sources;}

  \item{Services capable of scheduling and managing the execution of all of
  the above payloads, marshalling their results, and making them available to
  other parts of the system for analysis or further distribution.}

\end{itemize}

\subsection{Features to be tested}
\label{sec:feat2test}

\begin{itemize}

  \item{Execution of payloads described in \S\ref{sec:tasks};}
  \item{Persistence of all required data products;}
  \item{Scientific fidelity of those data products: do they satisfy the
  requirements described in \citeds{LSE-61}?}

\end{itemize}

\subsection{Features not to be tested}
\label{sec:featnot2test}

This document does not describe facilities for periodically generating or
collecting key performance metrics (KPMs), except insofar as those KPMs are
incidentally measured as part of executing the documented testcases. The KPMs
and the system being used to track KPMs and to ensure compliance with
documented requirements is described in \citeds{LDM-502}.

\subsection{Pass/fail criteria}
\label{sec:passfail}

The results of all tests will be assessed using the criteria described in
\citeds{LDM-503} \S4.

Note that, when executing pipelines, tasks or individual algorithms, any
unexplained or unexpected errors or warnings appearing in the associated log
or on screen output must be described in the documentation for the system
under test. Any warning or error for which this is not the case must be filed
as a software problem report and filed with the DMCCB.

\subsection{Suspension criteria and resumption requirements}
\label{suspension}

Refer to individual test cases where applicable.

\subsection{Naming convention}

All tests are named according to the pattern \textsc{prod-xx-yy} where:

\begin{description}[font=\normalfont\scshape]

  \item[prod]{The product under test. Relevant entries for this document are:
    \begin{description}[font=\normalfont\scshape,topsep=-1.0ex]
      \item[AG]{The Alert Generation payload and associated service}
      \item[AD]{The Alert Distribution payload and associated service}
      \item[AF]{The Alert Filtering service}
      \item[PFP]{The Precovery and Forced Photometry payload and associated
	      service}
      \item[MOPS]{The MOPS payload and associated service}
    \end{description}
  }
  \item[xx]{Test specification number (in increments of 10)}
  \item[yy]{Test case number (in increments of 5)}

\end{description}

%\input{specs/specs.tex}
%\input{cases/cases.tex}

\end{document}
